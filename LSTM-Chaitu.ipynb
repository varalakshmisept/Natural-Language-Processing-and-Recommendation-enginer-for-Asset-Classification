{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a37d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Bidirectional\n",
    "import config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle \n",
    "import sys\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "091f8f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestSplit(df,n):\n",
    "    \n",
    "    df1 = df['ASSET_CLASS'].value_counts().rename_axis('Assets').reset_index(name = 'counts')\n",
    "    df_new = df1[df1['counts']>=n] # Train Test split 75% - train\n",
    "    assets = list(df_new['Assets'])\n",
    "    dffiltered = df[df['ASSET_CLASS'].isin(assets)]\n",
    "    dffiltered['ASSET_CLASS_CODES'] = pd.Categorical(dffiltered['ASSET_CLASS'])\n",
    "    dffiltered['ASSET_CLASS_CODES'] = dffiltered['ASSET_CLASS_CODES'].cat.codes\n",
    "    \n",
    "    x = dffiltered['SPELL_CORRECTED']\n",
    "    y = pd.get_dummies(dffiltered['ASSET_CLASS_CODES']) \n",
    "\n",
    "    #buliding mapping dict from codes to Asset Classes\n",
    "    indexes_y = y.drop_duplicates().index\n",
    "    asset_classes = dffiltered.loc[indexes_y,\"ASSET_CLASS\"].values\n",
    "    asset_class_codes = dffiltered.loc[indexes_y,\"ASSET_CLASS_CODES\"].values\n",
    "    code_asset_class_mapping_dict = dict(zip(asset_class_codes,asset_classes))\n",
    "    \n",
    "    with open(config.code_asset_class_mapping_dict, 'wb') as f:\n",
    "        pickle.dump(code_asset_class_mapping_dict, f)\n",
    "    \n",
    "    #sanity check to determine whether the codes are being mapped correctly to the asset classes\n",
    "    #y_stack = y.stack()\n",
    "    #print(pd.Series(pd.Categorical(y_stack[y_stack!=0].index.get_level_values(1))))\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size = 0.20, stratify = y)\n",
    "    print(' Number of Assets ' + str(len(set(list(dffiltered['ASSET_CLASS'])))))\n",
    "    return X_train, X_test,  Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4570974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEmbeddingIndex():\n",
    "    print('Indexing word vectors.')\n",
    "    embeddings_index = {}\n",
    "    with open((config.utils_dir+config.glove_txt_300d)) as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff40854f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bilstm(X_train, X_test, Y_train, Y_test,wordembeddings):\n",
    "    np.random.seed(1234)\n",
    "    tf.random.set_seed(1234)\n",
    "    random.seed(1234)\n",
    "    \n",
    "    max_length_sentence = X_train.str.split().str.len().max()\n",
    "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',lower=True)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    #pickle dump word_index dictionary\n",
    "    with open(config.word_index_lstm, 'wb') as f:\n",
    "        pickle.dump(word_index, f)\n",
    "    \n",
    "    #pickle dump max_length_sentence\n",
    "    with open(config.max_length_sentence_lstm, 'wb') as f:\n",
    "        pickle.dump(max_length_sentence, f)\n",
    "    \n",
    "    with open(config.tokenizer_lstm, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    \n",
    "    \n",
    "    EMBEDDING_DIM=300\n",
    "    vocabulary_size=len(word_index)+1\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "    sequences_valid=tokenizer.texts_to_sequences(X_test)\n",
    "    X_train = pad_sequences(sequences_train,maxlen=max_length_sentence)\n",
    "    X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\n",
    "    y_train = np.asarray(Y_train)\n",
    "    y_val = np.asarray(Y_test)\n",
    "    #print(word_index)\n",
    "    \n",
    "    '''\n",
    "    print('Shape of data tensor:', X_train.shape)\n",
    "    print('Shape of data tensor:', X_val.shape)\n",
    "    print('Shape of data tensor:', y_train.shape)\n",
    "    print('Shape of data tensor:', y_val.shape)\n",
    "    \n",
    "    print(X_train)\n",
    "    print(\"*\"*100)\n",
    "    print(X_val)\n",
    "    print(\"*\"*100)\n",
    "    print(y_train)\n",
    "    print(\"*\"*100)\n",
    "    print(y_val)\n",
    "    '''\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if(word in wordembeddings.keys()):\n",
    "            embedding_vector = wordembeddings[word]\n",
    "            if len(embedding_vector)==0: #if array is empty\n",
    "                embedding_vector = wordembeddings[word.title()]\n",
    "                if len(embedding_vector)==0:\n",
    "                    embedding_vector = wordembeddings[word.upper()]\n",
    "                    if len(embedding_vector)==0:\n",
    "                        embedding_vector = np.array([round(np.random.rand(),8) for i in range(0,300)])\n",
    "                        \n",
    "        else:\n",
    "            #print(\"WORD NOT IN DICT\",word)\n",
    "            embedding_vector = np.array([round(np.random.rand(),8) for i in range(0,300)])\n",
    "            \n",
    "        if len(embedding_vector)!=0:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False) #Try with True\n",
    "    \n",
    "    \n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "    model = (Embedding(vocabulary_size, EMBEDDING_DIM, input_length=max_length_sentence,weights=[embedding_matrix]))(inputs)\n",
    "    \n",
    "    model = (LSTM(64))(model)\n",
    "    model = (Dense(900, activation='relu'))(model)\n",
    "    model = (Dense(400, activation='relu'))(model)\n",
    "    model = (Dense(250, activation='relu'))(model)\n",
    "    model = (Dense(204, activation='softmax'))(model)\n",
    "    model = Model(inputs=inputs,outputs=model)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "    hist_adam = model.fit(X_train, y_train, batch_size=1000, epochs=200, verbose=1, validation_data=(X_val, y_val),callbacks=callbacks)     #!!!!!!!!!!!!!!!!!!!!!!!CHANGE BATCH SIZE TO 1000 #change epochs to 200\n",
    "    model.save(config.lstm_prepocessed_dataset1_chai)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    print(y_pred)\n",
    "    \n",
    "    y_val_class = pd.DataFrame(y_val).idxmax(axis=1)\n",
    "    print(y_val_class)\n",
    "    \n",
    "    y_val_class_argmax = np.argmax(y_val,axis=1)\n",
    "    y_pred_class_argmax = np.argmax(y_pred,axis=1)\n",
    "    \n",
    "    y_pred_class = pd.DataFrame(y_pred).idxmax(axis=1)\n",
    "    print(y_pred_class)\n",
    "    \n",
    "    \n",
    "    print(classification_report(y_val_class, y_pred_class))\n",
    "    \n",
    "    plt.suptitle('Optimizer : Adam', fontsize=10)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    plt.plot(hist_adam.history['loss'], color='b', label='Training Loss')\n",
    "    plt.plot(hist_adam.history['val_loss'], color='r', label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig('/home/ubuntu/asset_classification/results/lstm_model_dataset1_preprocessed_chai.png')\n",
    "    \n",
    "    tf.keras.utils.plot_model(model, to_file=config.lstm_architecture, show_shapes=True)\n",
    "    \n",
    "    return(y_pred,y_val_class,y_pred_class,y_val_class_argmax,y_pred_class_argmax)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ce04483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BUSINESS_UNIT</th>\n",
       "      <th>PSC_CODE</th>\n",
       "      <th>FUND_SUBOBJCLASS</th>\n",
       "      <th>OBJ_CODE</th>\n",
       "      <th>SUB_OBJ_DESCR</th>\n",
       "      <th>ORDER_DATE</th>\n",
       "      <th>ORDER_TITLE</th>\n",
       "      <th>LINE_DESCRIPTION</th>\n",
       "      <th>VENDOR_NAME</th>\n",
       "      <th>VENDOR_COUNTRY</th>\n",
       "      <th>COST</th>\n",
       "      <th>ASSET_CLASS</th>\n",
       "      <th>ASSET_CLASS_DESCRIPTION</th>\n",
       "      <th>text_fields</th>\n",
       "      <th>PROCESSED_TEXT_FIELDS</th>\n",
       "      <th>SPELL_CORRECTED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOCATION 81</td>\n",
       "      <td>7290</td>\n",
       "      <td>4161</td>\n",
       "      <td>GRANTS/CONT/SUBSIDY</td>\n",
       "      <td>VALUE-ADDED TAXES</td>\n",
       "      <td>Mon Apr 22 2019 07:05:43 GMT-0400 (EDT)</td>\n",
       "      <td>transformers warehouse location fap</td>\n",
       "      <td>transformers outlet made plastic case carrying...</td>\n",
       "      <td>RON SITON</td>\n",
       "      <td>ISR</td>\n",
       "      <td>700.6405</td>\n",
       "      <td>39300</td>\n",
       "      <td>TRANSFORMER</td>\n",
       "      <td>transformers warehouse location fap transforme...</td>\n",
       "      <td>transformers warehouse location fap transforme...</td>\n",
       "      <td>transformers warehouse location fap transforme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOCATION 81</td>\n",
       "      <td>7290</td>\n",
       "      <td>3123</td>\n",
       "      <td>EQUIPMENT</td>\n",
       "      <td>HOUSEHOLD FURNISHING</td>\n",
       "      <td>Mon Apr 22 2019 07:05:43 GMT-0400 (EDT)</td>\n",
       "      <td>transformers warehouse location fap</td>\n",
       "      <td>transformers outlet made plastic case carrying...</td>\n",
       "      <td>RON SITON</td>\n",
       "      <td>ISR</td>\n",
       "      <td>4121.4146</td>\n",
       "      <td>39300</td>\n",
       "      <td>TRANSFORMER</td>\n",
       "      <td>transformers warehouse location fap transforme...</td>\n",
       "      <td>transformers warehouse location fap transforme...</td>\n",
       "      <td>transformers warehouse location fap transforme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOCATION 169</td>\n",
       "      <td>6120</td>\n",
       "      <td>4161</td>\n",
       "      <td>GRANTS/CONT/SUBSIDY</td>\n",
       "      <td>VALUE-ADDED TAXES</td>\n",
       "      <td>Thu Apr 25 2019 11:23:35 GMT-0400 (EDT)</td>\n",
       "      <td>gso icass stepdown transformers fap use</td>\n",
       "      <td>stepdown transformer full loadable primary vol...</td>\n",
       "      <td>Cosmos International Building Materials LLC</td>\n",
       "      <td>ARE</td>\n",
       "      <td>447.1821</td>\n",
       "      <td>39300</td>\n",
       "      <td>TRANSFORMER</td>\n",
       "      <td>gso icass stepdown transformers fap use stepdo...</td>\n",
       "      <td>gso ass stepdown transformers fap use stepdown...</td>\n",
       "      <td>so ass stepson transformers fap use stepson tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LOCATION 169</td>\n",
       "      <td>6120</td>\n",
       "      <td>3123</td>\n",
       "      <td>EQUIPMENT</td>\n",
       "      <td>HOUSEHOLD FURNISHING</td>\n",
       "      <td>Thu Apr 25 2019 11:23:35 GMT-0400 (EDT)</td>\n",
       "      <td>gso icass stepdown transformers fap use</td>\n",
       "      <td>stepdown transformer full loadable primary vol...</td>\n",
       "      <td>Cosmos International Building Materials LLC</td>\n",
       "      <td>ARE</td>\n",
       "      <td>8943.6428</td>\n",
       "      <td>39300</td>\n",
       "      <td>TRANSFORMER</td>\n",
       "      <td>gso icass stepdown transformers fap use stepdo...</td>\n",
       "      <td>gso ass stepdown transformers fap use stepdown...</td>\n",
       "      <td>so ass stepson transformers fap use stepson tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOCATION 78</td>\n",
       "      <td>6120</td>\n",
       "      <td>2675</td>\n",
       "      <td>SUPPLIES &amp; MATERIALS</td>\n",
       "      <td>RESIDENTIAL SUPPL/FU</td>\n",
       "      <td>Tue Mar 26 2019 14:53:15 GMT-0400 (EDT)</td>\n",
       "      <td>transformer step down priority</td>\n",
       "      <td>kohler cie fabrication transformateurs transfo...</td>\n",
       "      <td>KOHLER &amp; CIE</td>\n",
       "      <td>CHE</td>\n",
       "      <td>5790.5337</td>\n",
       "      <td>39300</td>\n",
       "      <td>TRANSFORMER</td>\n",
       "      <td>transformer step down priority kohler cie fabr...</td>\n",
       "      <td>transformer step down priority kohler cie fabr...</td>\n",
       "      <td>transformer step down priority kohler cie fabr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  BUSINESS_UNIT PSC_CODE FUND_SUBOBJCLASS              OBJ_CODE  \\\n",
       "0   LOCATION 81     7290             4161   GRANTS/CONT/SUBSIDY   \n",
       "1   LOCATION 81     7290             3123             EQUIPMENT   \n",
       "2  LOCATION 169     6120             4161   GRANTS/CONT/SUBSIDY   \n",
       "3  LOCATION 169     6120             3123             EQUIPMENT   \n",
       "4   LOCATION 78     6120             2675  SUPPLIES & MATERIALS   \n",
       "\n",
       "          SUB_OBJ_DESCR                               ORDER_DATE  \\\n",
       "0     VALUE-ADDED TAXES  Mon Apr 22 2019 07:05:43 GMT-0400 (EDT)   \n",
       "1  HOUSEHOLD FURNISHING  Mon Apr 22 2019 07:05:43 GMT-0400 (EDT)   \n",
       "2     VALUE-ADDED TAXES  Thu Apr 25 2019 11:23:35 GMT-0400 (EDT)   \n",
       "3  HOUSEHOLD FURNISHING  Thu Apr 25 2019 11:23:35 GMT-0400 (EDT)   \n",
       "4  RESIDENTIAL SUPPL/FU  Tue Mar 26 2019 14:53:15 GMT-0400 (EDT)   \n",
       "\n",
       "                               ORDER_TITLE  \\\n",
       "0      transformers warehouse location fap   \n",
       "1      transformers warehouse location fap   \n",
       "2  gso icass stepdown transformers fap use   \n",
       "3  gso icass stepdown transformers fap use   \n",
       "4           transformer step down priority   \n",
       "\n",
       "                                    LINE_DESCRIPTION  \\\n",
       "0  transformers outlet made plastic case carrying...   \n",
       "1  transformers outlet made plastic case carrying...   \n",
       "2  stepdown transformer full loadable primary vol...   \n",
       "3  stepdown transformer full loadable primary vol...   \n",
       "4  kohler cie fabrication transformateurs transfo...   \n",
       "\n",
       "                                   VENDOR_NAME VENDOR_COUNTRY       COST  \\\n",
       "0                                    RON SITON            ISR   700.6405   \n",
       "1                                    RON SITON            ISR  4121.4146   \n",
       "2  Cosmos International Building Materials LLC            ARE   447.1821   \n",
       "3  Cosmos International Building Materials LLC            ARE  8943.6428   \n",
       "4                                 KOHLER & CIE            CHE  5790.5337   \n",
       "\n",
       "  ASSET_CLASS ASSET_CLASS_DESCRIPTION  \\\n",
       "0       39300             TRANSFORMER   \n",
       "1       39300             TRANSFORMER   \n",
       "2       39300             TRANSFORMER   \n",
       "3       39300             TRANSFORMER   \n",
       "4       39300             TRANSFORMER   \n",
       "\n",
       "                                         text_fields  \\\n",
       "0  transformers warehouse location fap transforme...   \n",
       "1  transformers warehouse location fap transforme...   \n",
       "2  gso icass stepdown transformers fap use stepdo...   \n",
       "3  gso icass stepdown transformers fap use stepdo...   \n",
       "4  transformer step down priority kohler cie fabr...   \n",
       "\n",
       "                               PROCESSED_TEXT_FIELDS  \\\n",
       "0  transformers warehouse location fap transforme...   \n",
       "1  transformers warehouse location fap transforme...   \n",
       "2  gso ass stepdown transformers fap use stepdown...   \n",
       "3  gso ass stepdown transformers fap use stepdown...   \n",
       "4  transformer step down priority kohler cie fabr...   \n",
       "\n",
       "                                     SPELL_CORRECTED  \n",
       "0  transformers warehouse location fap transforme...  \n",
       "1  transformers warehouse location fap transforme...  \n",
       "2  so ass stepson transformers fap use stepson tr...  \n",
       "3  so ass stepson transformers fap use stepson tr...  \n",
       "4  transformer step down priority kohler cie fabr...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(config.datasets_dir+config.final_preprocessed)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a62f025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.replace(np.nan, '', regex = True)\n",
    "df[\"SPELL_CORRECTED\"].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8717d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-a8d1a8c136fe>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dffiltered['ASSET_CLASS_CODES'] = pd.Categorical(dffiltered['ASSET_CLASS'])\n",
      "<ipython-input-9-a8d1a8c136fe>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dffiltered['ASSET_CLASS_CODES'] = dffiltered['ASSET_CLASS_CODES'].cat.codes\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/asset_classification_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3445: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = trainTestSplit(df,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614d1f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09174fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c20d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed732e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,X_test.shape,Y_train.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf02def",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordembeddings = generateEmbeddingIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e7416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred,y_val_class,y_pred_class,y_val_class_argmax,y_pred_class_argmax = bilstm(X_train, X_test, Y_train, Y_test, wordembeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e8b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_class_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c30cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ea640",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d73eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('/home/ubuntu/asset_classification/results/lstm_model_dataset1_preprocessed_chai_new.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cfef7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
