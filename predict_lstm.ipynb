{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75a4b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pickle\n",
    "import config\n",
    "import enchant\n",
    "from spellchecker import SpellChecker\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "stop_words =set(stopwords.words('english'))\n",
    "english_dict = enchant.Dict(\"en_US\")\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cca8d2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Run this part to get the embeddings_index keys, the first time.\\ndef generateEmbeddingIndex():\\n    print('Indexing word vectors.')\\n    embeddings_index = {}\\n    with open((config.utils_dir+config.glove_txt_300d)) as f:\\n        for line in f:\\n            word, coefs = line.split(maxsplit=1)\\n            coefs = np.fromstring(coefs, 'f', sep=' ')\\n            embeddings_index[word] = coefs\\n\\n    print('Found %s word vectors.' % len(embeddings_index))\\n\\n    with open(config.glove_embeddings_dict, 'wb') as f:\\n        pickle.dump(embeddings_index, f)\\n\\n    with open(config.glove_embeddings_dict_keys, 'wb') as f:\\n        pickle.dump(list(embeddings_index.keys()), f)\\n    \\ngenerateEmbeddingIndex()\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Run this part to get the embeddings_index keys, the first time.\n",
    "def generateEmbeddingIndex():\n",
    "    print('Indexing word vectors.')\n",
    "    embeddings_index = {}\n",
    "    with open((config.utils_dir+config.glove_txt_300d)) as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    with open(config.glove_embeddings_dict, 'wb') as f:\n",
    "        pickle.dump(embeddings_index, f)\n",
    "\n",
    "    with open(config.glove_embeddings_dict_keys, 'wb') as f:\n",
    "        pickle.dump(list(embeddings_index.keys()), f)\n",
    "    \n",
    "generateEmbeddingIndex()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a597210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(order_title,line_description):\n",
    "    test_string = order_title+' '+line_description\n",
    "    x = re.sub('[^a-zA-Z]', ' ', test_string)\n",
    "    x = x.split(' ')\n",
    "    x =[word.lower() for word in x if not word in stop_words and len(word)>2]\n",
    "    test_sentence = ' '.join(x)\n",
    "    \n",
    "    #loading the word glove dictionary - uncomment this later\n",
    "\n",
    "    with open(config.glove_embeddings_dict_keys, 'rb') as f:\n",
    "        available_embeddings_words = pickle.load(f) #dump and load these keys\n",
    "\n",
    "    \n",
    "    for index,word in enumerate(x):\n",
    "        if word not in available_embeddings_words:\n",
    "            res = [word[i: j] for i in range(len(word)) for j in range(i + 1, len(word) + 1) if len(word[i: j].strip())>2 and english_dict.check(word[i: j])]\n",
    "            test_sentence = test_sentence.replace(word, ' '.join(res))\n",
    "            \n",
    "    new_vocab = list(set(test_sentence.split(' ')))\n",
    "    for word in new_vocab:\n",
    "        if not english_dict.check(word):\n",
    "            test_sentence = test_sentence.replace(word,spell.correction(word)) #replace incorrect word with the corrected word\n",
    "\n",
    "    return(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e053bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  main(order_title,line_description):\n",
    "    pre_processed_sentence = [pre_process(order_title,line_description)]\n",
    "    #pre processed sentence. Return if required\n",
    "    #print(pre_processed_sentence)\n",
    "    \n",
    "    with open(config.tokenizer_lstm, 'rb') as f:\n",
    "        tokenizer_lstm = pickle.load(f)\n",
    "            \n",
    "    with open(config.max_length_sentence_lstm, 'rb') as f:\n",
    "        max_length_sentence_lstm = pickle.load(f)\n",
    "    \n",
    "    with open(config.code_asset_class_mapping_dict, 'rb') as f:\n",
    "        code_asset_class_mapping_dict = pickle.load(f)\n",
    "    \n",
    "    #split_pre_processed_sentence = pre_processed_sentence.split(\" \")\n",
    "    test_string_to_sequence = tokenizer_lstm.texts_to_sequences(pre_processed_sentence)\n",
    "    test_string_final_to_predict = pad_sequences(test_string_to_sequence,maxlen=max_length_sentence_lstm)\n",
    "        \n",
    "    lstm_model = load_model(config.lstm_prepocessed_dataset1_chai)\n",
    "    \n",
    "    probs = lstm_model.predict(test_string_final_to_predict)\n",
    "    top_codes = list((-probs).argsort()[:,:5])[0]\n",
    "    top_preds_and_probs = {}\n",
    "    \n",
    "    for asset_class_code in top_codes:\n",
    "        top_preds_and_probs[code_asset_class_mapping_dict[asset_class_code]] = probs[:,asset_class_code][0]\n",
    "    \n",
    "    return(top_preds_and_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3bec363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'39300': 0.9922735,\n",
       " '44100': 0.006911354,\n",
       " '90000': 0.00058012776,\n",
       " '44110': 0.00013391404,\n",
       " '32805': 5.1261435e-05}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main('transformers% warehouse#123 location$ fap9989','transformers outlet made pastic case carrying transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11696c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
